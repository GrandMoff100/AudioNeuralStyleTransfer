{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioStyleTransferModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_xVhg4i3eCH2CIuRTJQ0G6c3He46m_GN",
      "authorship_tag": "ABX9TyMHLhfvEgj3d5ywnsuqeuEM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install noisereduce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G4DXNaaSPYw",
        "outputId": "6d1da60f-dfe4-489b-a794-9e179ffb9111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting noisereduce\n",
            "  Downloading noisereduce-2.0.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from noisereduce) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from noisereduce) (1.21.6)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (from noisereduce) (0.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from noisereduce) (4.64.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from noisereduce) (3.2.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (0.3.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (2.1.9)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (0.10.3.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (0.51.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (21.3)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa->noisereduce) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa->noisereduce) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa->noisereduce) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa->noisereduce) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->noisereduce) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->noisereduce) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->noisereduce) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->noisereduce) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->noisereduce) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->noisereduce) (2022.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->noisereduce) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa->noisereduce) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->noisereduce) (2.21)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->noisereduce) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->noisereduce) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->noisereduce) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->noisereduce) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->noisereduce) (1.15.0)\n",
            "Installing collected packages: noisereduce\n",
            "Successfully installed noisereduce-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "from sys import argv\n",
        "import torchvision.transforms as transforms\n",
        "import copy\n",
        "import librosa\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "\t\tdef __init__(self):\n",
        "\t\t\tsuper(CNNModel, self).__init__()\n",
        "\t\t\tself.cnn1 = nn.Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
        "\t\t\t#self.nl1 = nn.ReLU()\n",
        "\t\t\t#self.pool1 = nn.AvgPool1d(kernel_size=5)\n",
        "\t\t\t#self.fc1 = nn.Linear(4096*2500,2**5)\n",
        "\t\t\t#self.nl3 = nn.ReLU()\n",
        "\t\t\t#self.fc2 = nn.Linear(2**10,2**5)\n",
        "\t\t\n",
        "\t\tdef forward(self, x):\n",
        "\t\t\tout = self.cnn1(x)\n",
        "\t\t\t#out = self.nl1(out)\n",
        "\t\t\t#out = self.pool1(out)\n",
        "\t\t\tout = out.view(out.size(0),-1)\n",
        "\t\t\t#out = self.fc1(out)\n",
        "\t\t\t#out = self.nl3(out)\n",
        "\t\t\t#out = self.fc2(out)\n",
        "\t\t\treturn out\n",
        "\n",
        "\n",
        "class GramMatrix(nn.Module):\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\ta, b, c = input.size()  # a=batch size(=1)\n",
        "        # b=number of feature maps\n",
        "        # (c,d)=dimensions of a f. map (N=c*d)\n",
        "\t\tfeatures = input.view(a * b, c)  # resise F_XL into \\hat F_XL\n",
        "\t\tG = torch.mm(features, features.t())  # compute the gram product\n",
        "        # we 'normalize' the values of the gram matrix\n",
        "        # by dividing by the number of element in each feature maps.\n",
        "\t\treturn G.div(a * b * c)\n",
        "\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "\n",
        "\tdef __init__(self, target, weight):\n",
        "\t\tsuper(StyleLoss, self).__init__()\n",
        "\t\tself.target = target.detach() * weight\n",
        "\t\tself.weight = weight\n",
        "\t\tself.gram = GramMatrix()\n",
        "\t\tself.criterion = nn.MSELoss()\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\tself.output = input.clone()\n",
        "\t\tself.G = self.gram(input)\n",
        "\t\tself.G.mul_(self.weight)\n",
        "\t\tself.loss = self.criterion(self.G, self.target)\n",
        "\t\treturn self.output\n",
        "\n",
        "\tdef backward(self,retain_graph=True):\n",
        "\t\tself.loss.backward(retain_graph=retain_graph)\n",
        "\t\treturn self.loss\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\t# print('Enter the names of SCRIPT, Content audio, Style audio')\n",
        "    style_audio_name = \"/content/drive/Shareddrives/Beural Style Transfer/NewMusic/Cage The Elephant/Cage The Elephant - Ready To Let Go.mp3\"\n",
        "    content_audio_name = \"/content/drive/Shareddrives/Beural Style Transfer/NewMusic/Cage The Elephant/Cage The Elephant - Come a Little Closer.mp3\"\n",
        "\n",
        "    # USING LIBROSA\n",
        "    N_FFT=2048\n",
        "    def read_audio_spectum(filename):\n",
        "        x, fs = librosa.load(filename, duration=58.04) # Duration=58.05 so as to make sizes convenient\n",
        "        S = librosa.stft(x, N_FFT)\n",
        "        p = np.angle(S)\n",
        "        S = np.log1p(np.abs(S))  \n",
        "        return S, fs\n",
        "\n",
        "    style_audio, style_sr = read_audio_spectum(style_audio_name)\n",
        "    content_audio, content_sr = read_audio_spectum(content_audio_name)\n",
        "\n",
        "    if(content_sr == style_sr):\n",
        "        print('Sampling Rates are same')\n",
        "    else:\n",
        "        print('Sampling rates are not same')\n",
        "        exit()\n",
        "\n",
        "    num_samples=style_audio.shape[1]\t\n",
        "        \n",
        "    style_audio = style_audio.reshape([1,1025,num_samples])\n",
        "    content_audio = content_audio.reshape([1,1025,num_samples])\n",
        "\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        style_float = Variable((torch.from_numpy(style_audio)).cuda())\n",
        "        content_float = Variable((torch.from_numpy(content_audio)).cuda())\t\n",
        "    else:\n",
        "        style_float = Variable(torch.from_numpy(style_audio))\n",
        "        content_float = Variable(torch.from_numpy(content_audio))\n",
        "    #style_float = style_float.unsqueeze(0)\n",
        "\n",
        "    #style_float = style_float.view([1025,1,2500])\n",
        "\n",
        "    '''\n",
        "    print(style_float.size())\n",
        "    exit()\n",
        "    '''\n",
        "    #style_float = style_float.unsqueeze(0)\n",
        "    #content_float = content_float.unsqueeze(0)\n",
        "    #content_float = content_float.reshape(1025,1,2500)\n",
        "\n",
        "    #content_float = content_float.unsqueeze(0)\n",
        "    #content_float = content_float.squeeze(0)\n",
        "\n",
        "    cnn = CNNModel()\n",
        "    if torch.cuda.is_available():\n",
        "        cnn = cnn.cuda()\n",
        "    style_layers_default = ['conv_1']\n",
        "\n",
        "    style_weight=10000\n",
        "\n",
        "    def get_style_model_and_losses(cnn, style_float,style_weight=style_weight, style_layers=style_layers_default): #STYLE WEIGHT\n",
        "        \n",
        "        cnn = copy.deepcopy(cnn)\n",
        "        style_losses = []\n",
        "        model = nn.Sequential()  # the new Sequential module network\n",
        "        gram = GramMatrix()  # we need a gram module in order to compute styOzzy Osbourne - Trap Door.mp3le targets\n",
        "        if torch.cuda.is_available():\n",
        "            model = model.cuda()\n",
        "            gram = gram.cuda()\n",
        "\n",
        "        name = 'conv_1'\n",
        "        model.add_module(name, cnn.cnn1)\n",
        "        if name in style_layers:\n",
        "            target_feature = model(style_float).clone()\n",
        "            target_feature_gram = gram(target_feature)\n",
        "            style_loss = StyleLoss(target_feature_gram, style_weight)\n",
        "            model.add_module(\"style_loss_1\", style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "        #name = 'pool_1'\n",
        "        #model.add_module(name, cnn.pool1)\n",
        "\n",
        "        '''name = 'fc_1'\n",
        "        model.add_module(name, cnn.fc1)\n",
        "\n",
        "        name = 'nl_9'\n",
        "        model.add_module(name, cnn.nl9)\n",
        "\n",
        "        name = 'fc_2'\n",
        "        model.add_module(name, cnn.fc2)'''\n",
        "\n",
        "        return model, style_losses\n",
        "\n",
        "\n",
        "    input_float = content_float.clone()\n",
        "    #input_float = Variable(torch.randn(content_float.size())).type(torch.FloatTensor)\n",
        "\n",
        "    learning_rate_initial = 0.03\n",
        "\n",
        "    def get_input_param_optimizer(input_float):\n",
        "        input_param = nn.Parameter(input_float.data)\n",
        "        #optimizer = optim.Adagrad([input_param], lr=learning_rate_initial, lr_decay=0.0001,weight_decay=0)\n",
        "        optimizer = optim.Adam([input_param], lr=learning_rate_initial, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
        "        return input_param, optimizer\n",
        "\n",
        "    num_steps= 1\n",
        "\n",
        "    def run_style_transfer(cnn, style_float, input_float, num_steps=num_steps, style_weight=style_weight): #STYLE WEIGHT, NUM_STEPS\n",
        "        print('Building the style transfer model..')\n",
        "        model, style_losses= get_style_model_and_losses(cnn, style_float, style_weight)\n",
        "        input_param, optimizer = get_input_param_optimizer(input_float)\n",
        "        print('Optimizing..')\n",
        "        run = [0]\n",
        "\n",
        "        while run[0] <= num_steps:\n",
        "            def closure():\n",
        "                # correct the values of updated input image\n",
        "                input_param.data.clamp_(0, 1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                model(input_param)\n",
        "                style_score = 0\n",
        "\n",
        "                for sl in style_losses:\n",
        "                    #print('sl is ',sl,' style loss is ',style_score)\n",
        "                    style_score += sl.backward()\n",
        "\n",
        "                run[0] += 1\n",
        "                if run[0] % 100 == 0:\n",
        "                    print(\"run {}:\".format(run))\n",
        "                    print('Style Loss : {}'.format(style_score.data)) #CHANGE 4->8 \n",
        "                    print()\n",
        "\n",
        "                return style_score\n",
        "\n",
        "\n",
        "            optimizer.step(closure)\n",
        "        input_param.data.clamp_(0, 1)\n",
        "        return input_param.data\n",
        "        \n",
        "    output = run_style_transfer(cnn, style_float, input_float)\n",
        "    if torch.cuda.is_available():\n",
        "        output = output.cpu()\n",
        "\n",
        "    #output = output.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    output = output.numpy()\n",
        "    #print(output.shape)\n",
        "    #output = output.resize([1025,2500])\n",
        "\n",
        "    N_FFT=2048\n",
        "    a = np.zeros_like(output)\n",
        "    a = np.exp(output) - 1\n",
        "\n",
        "    # This code is supposed to do phase reconstruction\n",
        "    p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "    for i in range(1):\n",
        "        S = a * np.exp(1j*p)\n",
        "        x = librosa.istft(S)\n",
        "        p = np.angle(librosa.stft(x, N_FFT))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_wE1OysSPdv",
        "outputId": "413ac5a9-d047-45bc-f7dc-f290bcdb95e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling Rates are same\n",
            "Building the style transfer model..\n",
            "Optimizing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import noisereduce as nr\n",
        "\n",
        "OUTPUT_FILENAME = content_audio_name.split(\"/\")[-1]+'_s'+style_audio_name.split(\"/\")[-1]+'_sw'+str(style_weight)+'_k3s1p9.wav'\n",
        "\n",
        "# Write out audio as 24bit PCM WAV\n",
        "\n",
        "# perform noise reduction\n",
        "reduced_noise = nr.reduce_noise(y=x, sr=style_sr * 1.2 // 1, stationary=True, freq_mask_smooth_hz=52)\n",
        "sf.write(OUTPUT_FILENAME, reduced_noise, style_sr, subtype='PCM_24')\n"
      ],
      "metadata": {
        "id": "7uR_kdh2WmIY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}